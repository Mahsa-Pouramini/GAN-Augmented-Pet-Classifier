{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noticed-sewing",
   "metadata": {},
   "source": [
    "# GANs Augmented Pet Classifier\n",
    "<div style=\"text-align:center\"><img src=\"Figures/training_sequence.gif\" /></div>\n",
    "\n",
    "###Towards Fine-grained Image Classification withGenerative Adversarial Networks and FacialLandmark Detection\n",
    "Mahdi Darvish, Mahsa Pouramini, Hamid Bahador\n",
    "\n",
    "arixv ;llinkkk\n",
    "\n",
    "Abstract: *Fine-grained   classification   remains   a   challengingtask  because  distinguishing  categories  needs  learning  complexand  local  differences.  Diversity  in  the  pose,  scale,  and  positionof  objects  in  an  image  makes  the  problem  even  more  difficult.Although  the  recent  Vision  Transformer  models  achieve  highperformance,  they  need  an  extensive  volume  of  input  data.  Toencounter this problem, we made the best use of GAN-based dataaugmentation  to  generate  extra  dataset  instances.  Oxford-IIITPets  was  our  dataset  of  choice  for  this  experiment.  It  consistsof  37  breeds  of  cats  and  dogs  with  variations  in  scale,  poses,and  lighting,  which  intensifies  the  difficulty  of  the  classificationtask.  Furthermore,  we  enhanced  the  performance  of  the  recentGenerative Adversarial Network (GAN), StyleGAN2-ADA modelto generate more realistic images while preventing overfitting tothe  training  set.  We  did  this  by  training  a  customized  versionof  MobileNetV2  to  predict  animal  facial  landmarks;  then,  wecropped  images  accordingly.  Lastly,  we  combined  the  syntheticimages  with  the  original  dataset  and  compared  our  proposedmethod with standard GANs augmentation and no augmentationwith  different  subsets  of  training  data.  We  validated  our  workby  evaluating  the  accuracy  of  fine-grained  image  classificationon the recent Vision Transformer (ViT) Model. *\n",
    "\n",
    "# Results\n",
    "\n",
    "the evaluated RMSE of the trained MobileNetV2 model with\n",
    "and without landmark normalization:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"Figures/RMSE.PNG\" /></div>\n",
    "\n",
    "The measured accuracy of the used model and FID for three different dataset conditions (Original, augmented, and augmented-cropped) in data regimes of 10, 50, and 100 percent:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"Figures/FID.PNG\" /></div>\n",
    "\n",
    "Comparison between synthetic and authentic images. This figure show (a) the original data,(b) and (c) generated images on\n",
    "the whole dataset, cropped and uncropped, respectively. (d) cropped images on 50%, (e) uncropped images generated on 50%\n",
    "subset and finally (f) and (g), cropped and uncropped images result of training on only 10% of the data. These qualitative\n",
    "visualizations prove the effectiveness and the interpretability of the method.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"Figures/result's pic.PNG\" /></div>\n",
    "\n",
    "Finally, the charts explain the accuracy of the used model and FID for three different dataset conditions (Original, augmented, and cropped-augmented ) in data regimes of 10, 50, and 100 percent:\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"Figures/charts.PNG\" /></div>\n",
    "\n",
    "# Pre-Trained Models\n",
    "\n",
    "### StyleGAN2-ADA trained on cropped pets dataset \n",
    "\n",
    "\n",
    "\n",
    "| Subset | Kimg | FID  | Acc on Vit | Model link | TFRecords |\n",
    "|--------|------|------|------------|------------|-----------|\n",
    "| 10%    | 5120 | 49.4 | 68.55      | 250 Mb     |           |\n",
    "| 50%    | 5120 | 22.3 | 91.73      | 250 Mb     |           |\n",
    "| 100%   | 5120 | 14.1 | 96.28      | 250 Mb     |           |\n",
    "\n",
    "\n",
    "### StyleGAN2-ADA trained on not cropped pets dataset \n",
    "\n",
    "| Subset | Kimg | FID  | Acc on Vit | Model link | TFRecords |\n",
    "|--------|------|------|------------|------------|-----------|\n",
    "| 10%    | 5120 | 71.1 | 63.32      | 250 Mb     |           |\n",
    "| 50%    | 5120 | 36.4 | 88.70      | 250 Mb     |           |\n",
    "| 100%   | 5120 | 20.7 | 94.93      | 250 Mb     |           |\n",
    "\n",
    "# Getting started\n",
    "## Dataset\n",
    "\n",
    "The official dataset can be reached from:\n",
    "\n",
    "[Oxford-IIIT Pet dataset.](https://www.robots.ox.ac.uk/~vgg/data/pets/)\n",
    "\n",
    "The cropped dataset iis given in:\n",
    "\n",
    "[GDRIVE.](https://drive.google.com/drive/u/7/my-drive)\n",
    "\n",
    "## StyleGAN2-ADA Installation\n",
    "### Running Localy\n",
    "#### Data Preperation\n",
    "#### Training\n",
    "#### Generating Images with Pre-trained Models\n",
    " ### Running through Google Colab\n",
    "## Landmark Detection\n",
    "# Citation\n",
    "\n",
    "# Acknowledgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-devices",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
